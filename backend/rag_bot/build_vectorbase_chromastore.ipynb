{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca78e8a1",
   "metadata": {},
   "source": [
    "Installed Langchain:\n",
    "\n",
    "pip install -qU \"langchain[openai]\"\n",
    "\n",
    "pip install -U langchain-community\n",
    "\n",
    "pip install pypdf\n",
    "\n",
    "pip install sentence-transformers\n",
    "\n",
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc89176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as notebook_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "236d1de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vector DB rebuilt and saved to: c:\\Users\\varma\\clone-mysite\\my-site\\backend\\rag_bot\\chroma_store\n"
     ]
    }
   ],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "# Load PDF\n",
    "loader = PyPDFLoader(\"trainingdata.pdf\")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split into chunks\n",
    "splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "\n",
    "# Set path to save Chroma DB (robust across environments)\n",
    "persist_directory = os.path.abspath(\"chroma_store\")\n",
    "\n",
    "# Embed and persist\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/paraphrase-MiniLM-L3-v2\")\n",
    "\n",
    "db = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory,\n",
    "    collection_name=\"karteek_context\"\n",
    ")\n",
    "\n",
    "print(\"✅ Vector DB rebuilt and saved to:\", persist_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ed5815",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d1a1004f",
   "metadata": {},
   "source": [
    "The cells below is only for test inference, and do not need once deployed unless to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9dadaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context:  Karteek’s Technical skill stack include Programming using Python (NumPy, Pandas), SQL, \n",
      "JavaScript, TypeScript, C#, MS SQL, react.js, CSS, HTML. His skills in AI/ML include understanding \n",
      "and the ability to build and utilize  Neural networks, Clustering, Classification, Regression, \n",
      "Transformer models, Reinforcement Learning (DDQL) algorithms. He is keen at NLP and has \n",
      "worked on projects that include utilizing Sentiment analysis, Text summarization, GPT fine-tuning, \n",
      "BLEU/ROUGE/Perplexity metrics, Word2Vec, TextBlob, NLTK. To complement his abilities as a ML \n",
      "engineer he also learned to Visualize data where he learned to use tools like Tableau, Power BI, \n",
      "Matplotlib, Seaborn, ggplot. He has also learned some Data Engineering skills involving: AWS \n",
      "pipelines, graph-based processing and feature engineering. He also possesses skills in Computer \n",
      "Vision and can perform tasks like Object detection, facial recognition, real-time video analysis and \n",
      "much more. Tools: Git, CI/CD pipelines, Power Vision (for HMI) \n",
      " \n",
      "Karteek has implemented multiple Professional, Personal and Academics projects, which are: \n",
      "Flood Prediction using Transformer Models, AI Stock Trading Agent, Kar_Per, which is a \n",
      "personal chatbot, A Personal website, Human-Machine Interface (HMI) for EcoCar EV \n",
      "Challenge. A detailed description for each of his projects is as follows: \n",
      "Flood Prediction using Transformer Models: Karteek has been a critical part of the development \n",
      "of an AI-based flood risk predictor using satellite imagery, transformer ML models, and geospatial \n",
      "data to help urban planners prepare mitigation strategies. This project earned 3rd place in the \n",
      "Grainger Computing Innovation Prize at Illinois Tech and has won his team a cash prize of $5000. \n",
      "The project enabled him to work with GIS data for the first time and that experience was later on \n",
      "used to implement one of his Academic projects for a course named Online Network and Social \n",
      "Media Analysis. \n",
      "AI Stock Trading Agent is one his Machine learning projects for school where he Designed a \n",
      "Double Deep Q-Learning agent that used LSTM networks to identify trading patterns in TCS stock \n",
      "from the Indian stock market. To build an agent that made profits from trading he created multiple \n",
      "custom reward-based environments that were put in to trial and error to find the most optimize \n",
      "reward function that can trade and make profit. Although the project is a Reinforcement Learning \n",
      "based, he still implemented some constraints to make the agent guess less and limit the actions it \n",
      "can take to make a profitable trade. \n",
      "Eco-Car Challenge - Human-Machine Interface (HMI): Until his final semester in college Karteek \n",
      "was a part of the Eco Car EV challenge, a competition where selected teams from universities \n",
      "across the country compete with each other to build an autonomous driving vehicle. Karteek’s role \n",
      "in the team was to build interactive HMI screens using Power Vision, a tool specifically designed to \n",
      "develop and implement functions on the screen which we were using, which was sponsored by \n",
      "UICO. Along with that his duties also included Integration of car-to-screen communication via CAN \n",
      "bus and custom DBC files. And designing Real-time control logic and state flows while aiming to \n",
      "improve vehicle-user interaction.\n",
      "\n",
      "Karteek’s Technical skill stack include Programming using Python (NumPy, Pandas), SQL, \n",
      "JavaScript, TypeScript, C#, MS SQL, react.js, CSS, HTML. His skills in AI/ML include understanding \n",
      "and the ability to build and utilize  Neural networks, Clustering, Classification, Regression, \n",
      "Transformer models, Reinforcement Learning (DDQL) algorithms. He is keen at NLP and has \n",
      "worked on projects that include utilizing Sentiment analysis, Text summarization, GPT fine-tuning, \n",
      "BLEU/ROUGE/Perplexity metrics, Word2Vec, TextBlob, NLTK. To complement his abilities as a ML \n",
      "engineer he also learned to Visualize data where he learned to use tools like Tableau, Power BI, \n",
      "Matplotlib, Seaborn, ggplot. He has also learned some Data Engineering skills involving: AWS \n",
      "pipelines, graph-based processing and feature engineering. He also possesses skills in Computer \n",
      "Vision and can perform tasks like Object detection, facial recognition, real-time video analysis and \n",
      "much more. Tools: Git, CI/CD pipelines, Power Vision (for HMI) \n",
      " \n",
      "Karteek has implemented multiple Professional, Personal and Academics projects, which are: \n",
      "Flood Prediction using Transformer Models: Karteek has been a critical part of the development \n",
      "of an AI-based flood risk predictor using satellite imagery, transformer ML models, and geospatial \n",
      "data to help urban planners prepare mitigation strategies. This project earned 3rd place in the \n",
      "Grainger Computing Innovation Prize at Illinois Tech and has won his team a cash prize of $5000. \n",
      "The project enabled him to work with GIS data for the first time and that experience was later on \n",
      "used to implement one of his Academic projects for a course named Online Network and Social \n",
      "Media Analysis. \n",
      "AI Stock Trading Agent is one his Machine learning projects for school where he Designed a \n",
      "Double Deep Q-Learning agent that used LSTM networks to identify trading patterns in TCS stock \n",
      "from the Indian stock market. To build an agent that made profits from trading he created multiple \n",
      "custom reward-based environments that were put in to trial and error to find the most optimize \n",
      "reward function that can trade and make profit. Although the project is a Reinforcement Learning \n",
      "based, he still implemented some constraints to make the agent guess less and limit the actions it \n",
      "can take to make a profitable trade. \n",
      "Eco-Car Challenge - Human-Machine Interface (HMI): Until his final semester in college Karteek \n",
      "was a part of the Eco Car EV challenge, a competition where selected teams from universities \n",
      "across the country compete with each other to build an autonomous driving vehicle. Karteek’s role \n",
      "in the team was to build interactive HMI screens using Power Vision, a tool specifically designed to \n",
      "develop and implement functions on the screen which we were using, which was sponsored by \n",
      "UICO. Along with that his duties also included Integration of car-to-screen communication via CAN \n",
      "bus and custom DBC files. And designing Real-time control logic and state flows while aiming to \n",
      "improve vehicle-user interaction. \n",
      "Kar_Per the chat personal chatbot is his recent project that he built to showcase his interest in \n",
      "natural language processing. Kar_Per is a chatbot he named after him meaning Karteek Pericharla. \n",
      "The project is a part of creating his portfolio to showcase his skills on the internet. The chatbot has\n"
     ]
    }
   ],
   "source": [
    "# query = \"Tell me about his projects\"\n",
    "# retrieved_docs = db.similarity_search(query, k=2)\n",
    "\n",
    "# # Get the context as plain text\n",
    "# context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
    "\n",
    "# print(\"Context: \", context)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-phi2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
